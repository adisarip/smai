{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Aditya Saripalli  \n",
    "Roll No: 20173071  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import linalg as lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating complete feature space from training data.\n",
    "def create_word_vectors(training_data_path):\n",
    "    #feature space - containing all the distinct words from all the documents\n",
    "    bag_of_words = set()\n",
    "    #A dictionary of word vectors with key=filename and\n",
    "    #value=A Counter Object representing word vector of that file\n",
    "    d_word_vectors = {}\n",
    "    # A dictionary with key=\"class_label\" and \n",
    "    # value=list of file_indices (\"class_label/filename\")\n",
    "    d_class_files = defaultdict(list)\n",
    "    #get the sub directories list\n",
    "    dir_list = os.listdir(training_data_path)\n",
    "\n",
    "    for class_label in dir_list:\n",
    "        source_dir = training_data_path + \"/\" + class_label\n",
    "        file_paths = [source_dir+\"/\"+file  for file in os.listdir(source_dir)]\n",
    "        #Read the all files and create the feature space\n",
    "        for file_path in file_paths:\n",
    "            #ignoring unicode characters while opening the data file\n",
    "            data_file = open(file_path, \"r\", encoding='utf-8', errors='ignore')\n",
    "            word_vector = Counter();\n",
    "            for line in data_file.readlines():\n",
    "                #exracting only lower case words\n",
    "                line_tokens = re.findall(r\"\\b[A-Za-z][a-z]{2,26}\\b\", line)\n",
    "                line_tokens = [token.lower() for token in line_tokens]\n",
    "                line_tokens = list(set(line_tokens))\n",
    "                for word in line_tokens: word_vector[word] += 1\n",
    "                bag_of_words |= set(line_tokens)\n",
    "            data_file.close()\n",
    "            file_index = class_label + \"/\" + file_path.split(\"/\")[-1]\n",
    "            d_word_vectors[file_index] = word_vector\n",
    "            d_class_files[class_label].append(file_index)\n",
    "\n",
    "    return list(bag_of_words), d_word_vectors, d_class_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_word_freq_vectors(bag_of_words, d_word_vectors):\n",
    "    #dictionary of word frequency vectors\n",
    "    d_word_freq_vectors = {}\n",
    "    for key,value in d_word_vectors.items():\n",
    "        word_freq_vector = []\n",
    "        for word in bag_of_words:\n",
    "            word_freq_vector += [value[word]]\n",
    "        d_word_freq_vectors[key] = word_freq_vector\n",
    "\n",
    "    return d_word_freq_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating word frequency Matrix\n",
    "def create_freq_matrix(bag_of_words, d_word_freq_vectors):\n",
    "    #create the term frequency matrix from word frequency vectors\n",
    "    freq_term_matrix = np.array([])\n",
    "    file_index_array = np.array([])\n",
    "\n",
    "    #sort the dictionary on keys\n",
    "    for key,value in d_word_freq_vectors.items():\n",
    "        freq_term_matrix = np.array(value) if freq_term_matrix.size == 0 \\\n",
    "                                           else np.vstack((freq_term_matrix, np.array(value)))\n",
    "        file_index_array = np.array(key) if file_index_array.size == 0 \\\n",
    "                                           else np.vstack((file_index_array, np.array(key)))\n",
    "    \n",
    "    return freq_term_matrix, file_index_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding class labels to the word frequency matrix for ease of computation\n",
    "def add_class_data(tf_matrix, file_index_array):\n",
    "    #get the dimensions of the tfidf matrix\n",
    "    n_records, n_features = tf_matrix.shape\n",
    "    #adding the class labels\n",
    "    tf_matrix = np.insert(tf_matrix, n_features, '0', axis=1)\n",
    "    for index, file_index in enumerate(file_index_array):\n",
    "        class_label = file_index[0].split(\"/\")[0]\n",
    "        tf_matrix[index][n_features] = class_label\n",
    "\n",
    "    return tf_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating word vector for the input file.\n",
    "def create_word_vector(filename):\n",
    "    #ignoring unicode characters while opening the data file\n",
    "    data_file = open(filename, \"r\", encoding='utf-8', errors='ignore')\n",
    "    word_vector = Counter();\n",
    "    for line in data_file.readlines():\n",
    "        #exracting only lower case words\n",
    "        line_tokens = re.findall(r\"\\b[A-Za-z][a-z]{2,26}\\b\", line)\n",
    "        line_tokens = [token.lower() for token in line_tokens]\n",
    "        line_tokens = list(set(line_tokens))\n",
    "        for word in line_tokens: word_vector[word] += 1\n",
    "    data_file.close()\n",
    "\n",
    "    return word_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_word_freq_vector(bag_of_words, word_vector):\n",
    "    #dictionary of word frequency vectors\n",
    "    _word_freq_vector = []\n",
    "    for word in bag_of_words:\n",
    "        _word_freq_vector += [word_vector[word]]\n",
    "\n",
    "    return _word_freq_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_relevant_docs(file_path, bag_of_words, c_tf_matrix):\n",
    "    #create word frequency vectors for the given document\n",
    "    word_vector = create_word_vector(file_path)\n",
    "    _word_freq_vector = compute_word_freq_vector(bag_of_words, word_vector)\n",
    "    word_freq_vector = np.array(_word_freq_vector)\n",
    "\n",
    "    d_cosine_values = {}\n",
    "    for vector in c_tf_matrix:\n",
    "        w_vector = np.array(vector[:-1])\n",
    "        c_label = vector[-1]\n",
    "        cosine_value = np.dot(word_freq_vector, w_vector) / (lg.norm(word_freq_vector) * lg.norm(w_vector))\n",
    "        d_cosine_values[cosine_value] = c_label\n",
    "    \n",
    "    return d_cosine_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class label of the given document is: 2\n"
     ]
    }
   ],
   "source": [
    "##################################################################################\n",
    "#              Latent Semantic Analysis - Fetch Relevant Documents               #\n",
    "##################################################################################\n",
    "\n",
    "# Creating the feature space (Bag of all the words from the corpus)\n",
    "# and a dictionary of word vectors corresponding to each input file\n",
    "\n",
    "bag_of_words, d_word_vectors, d_class_files = create_word_vectors(\"./q2data/train\")\n",
    "class_labels = [float(label) for label in list(d_class_files.keys())]\n",
    "\n",
    "# compute the word frequency vectors\n",
    "d_word_freq_vectors = compute_word_freq_vectors(bag_of_words, d_word_vectors)\n",
    "\n",
    "#create the word frequency matrix of the complete data set\n",
    "tf_matrix, file_index_array = create_freq_matrix(bag_of_words, d_word_freq_vectors)\n",
    "\n",
    "c_tf_matrix = add_class_data(tf_matrix, file_index_array)\n",
    "\n",
    "#fetch the relevant document\n",
    "d_cosine_values = fetch_relevant_docs(\"./q2data/train/2/278.txt\", bag_of_words, c_tf_matrix)\n",
    "\n",
    "l_cosine_values = [key for key in d_cosine_values]\n",
    "l_cosine_values.sort(reverse=True)\n",
    "l_relevant_cosine_values = l_cosine_values[0:10]\n",
    "\n",
    "l_relevant_class_labels = [int(d_cosine_values[x]) for x in l_relevant_cosine_values]\n",
    "file_class_label, n_occ = Counter(l_relevant_class_labels).most_common(1)[0]\n",
    "\n",
    "print(\"Class label of the given document is: {}\".format(file_class_label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
